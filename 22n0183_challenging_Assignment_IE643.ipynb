{"cells":[{"cell_type":"markdown","metadata":{"id":"R3tvWOHiEXH5"},"source":["# YOLOv7 Challenge Submission Instructions\n","\n","instructions\n","## 1. Install Dependencies\n","\n","To set up the required environment, use the provided `environment.yml` file. Run the following code in your Python environment:\n","\n","conda env create -f environment.yml\n","\n","\n","This command creates a Conda environment with the specified dependencies.\n","\n","## 2. Download YOLOv7 Code and Pre-trained Model\n","\n","Download the YOLOv7 code and the pre-trained model checkpoint from the provided [Google Drive link](https://drive.google.com/drive/folders/1qC0BoUVZbRv3PkA8wj5_qmqjfkY0CZSb?usp=sharing).\n","\n","The pre-trained model checkpoint is available at the location:\n","\n","/path/to/yolov7/runs/train/yolov7-e6e57/weights/best.pt\n","\n","\n","## 3. Optional: Train the Model\n","\n","If you want to train the model on your own dataset, follow these steps:\n","\n","### a. Training Section\n","\n","Run the training section in the YOLOv7 code. Adjust the configuration in the code if needed. This section will train the YOLOv7 model.\n","\n","### b. Data Augmentation Section\n","\n","Run the data augmentation section in the YOLOv7 code. This step enhances the model's performance by augmenting the training data.\n","\n","## 4. Evaluation on Other Datasets\n","\n","If you want to evaluate the pre-trained model on another dataset, modify the test data image path in the configuration file:\n","\n","Edit the file at:\n","\n","/path/to/yolov7/data/data.yaml\n","\n","\n","Specify the location of the test data images in the configuration file.\n","\n","## 5. Creating File for Challenge Submission\n","\n","I wrote a program that creates a CSV file specifically designed for submitting challenging assignments on Kaggle. The code makes sure the file meets Kaggle's submission criteria, making it easy to share and evaluate my work on the platform.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"56aaujgPFTNG","outputId":"7a74b89a-12a5-43a6-c19a-3b8f436ed6b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting sahi\n","  Downloading sahi-0.11.15-py3-none-any.whl (105 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opencv-python<=4.8 (from sahi)\n","  Downloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: shapely>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from sahi) (2.0.2)\n","Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.10/dist-packages (from sahi) (4.66.1)\n","Requirement already satisfied: pillow>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from sahi) (9.4.0)\n","Collecting pybboxes==0.1.6 (from sahi)\n","  Downloading pybboxes-0.1.6-py3-none-any.whl (24 kB)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from sahi) (6.0.1)\n","Collecting fire (from sahi)\n","  Downloading fire-0.5.0.tar.gz (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting terminaltables (from sahi)\n","  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from sahi) (2.31.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sahi) (8.1.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pybboxes==0.1.6->sahi) (1.23.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->sahi) (1.16.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->sahi) (2.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->sahi) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->sahi) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->sahi) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->sahi) (2023.7.22)\n","Building wheels for collected packages: fire\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116933 sha256=714e588343cfdd34fc74de0a9e268b2c2197ed92269269aff28c6f21488bc86a\n","  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n","Successfully built fire\n","Installing collected packages: terminaltables, pybboxes, opencv-python, fire, sahi\n","  Attempting uninstall: opencv-python\n","    Found existing installation: opencv-python 4.8.0.76\n","    Uninstalling opencv-python-4.8.0.76:\n","      Successfully uninstalled opencv-python-4.8.0.76\n","Successfully installed fire-0.5.0 opencv-python-4.7.0.72 pybboxes-0.1.6 sahi-0.11.15 terminaltables-3.1.10\n"]}],"source":["!pip install sahi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vKqH-mLDIkMj"},"outputs":[],"source":["import os\n","from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","import fire\n","from PIL import Image\n","from sahi.utils.coco import Coco, CocoAnnotation, CocoCategory, CocoImage\n","from sahi.utils.file import save_json\n","from tqdm import tqdm\n","import json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ICgx6FviIkQC"},"outputs":[],"source":["NAME_TO_COCO_CATEGORY = {\n","    \"pedestrian\": {\"name\": \"pedestrian\", \"supercategory\": \"person\"},\n","    \"people\": {\"name\": \"people\", \"supercategory\": \"person\"},\n","    \"bicycle\": {\"name\": \"bicycle\", \"supercategory\": \"bicycle\"},\n","    \"car\": {\"name\": \"car\", \"supercategory\": \"car\"},\n","    \"van\": {\"name\": \"van\", \"supercategory\": \"truck\"},\n","    \"truck\": {\"name\": \"truck\", \"supercategory\": \"truck\"},\n","    \"tricycle\": {\"name\": \"tricycle\", \"supercategory\": \"motor\"},\n","    \"awning-tricycle\": {\"name\": \"awning-tricycle\", \"supercategory\": \"motor\"},\n","    \"bus\": {\"name\": \"bus\", \"supercategory\": \"bus\"},\n","    \"motor\": {\"name\": \"motor\", \"supercategory\": \"motor\"},\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kFlq2AVAJWG-"},"outputs":[],"source":["NAME_TO_CATEGORY_ID = {\n","    \"pedestrian\":\"0\",\n","    \"people\":\"1\",\n","    \"bicycle\":\"2\",\n","    \"car\":\"3\",\n","    \"van\":\"4\",\n","    \"truck\":\"5\",\n","    \"tricycle\":\"6\",\n","    \"awning-tricycle\":\"7\",\n","    \"bus\":\"8\",\n","    \"motor\":\"9\",\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bZG-33CZJWJ2"},"outputs":[],"source":["CATEGORY_ID_TO_NAME = {\n","    \"0\": \"pedestrian\",\n","    \"1\": \"people\",\n","    \"2\": \"bicycle\",\n","    \"3\": \"car\",\n","    \"4\": \"van\",\n","    \"5\": \"truck\",\n","    \"6\": \"tricycle\",\n","    \"7\": \"awning-tricycle\",\n","    \"8\": \"bus\",\n","    \"9\": \"motor\",\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HEvPRiJ0JWM0"},"outputs":[],"source":["def convert_box(size, box):\n","  # Convert VisDrone box to YOLO xywh box\n","  dw = 1. / size[0]\n","  dh = 1. / size[1]\n","  return (box[0] + box[2] / 2)*dw , (box[1] + box[3] / 2)*dh, box[2]*dw , box[3]*dh"]},{"cell_type":"markdown","metadata":{"id":"QZfdyRh-TzX5"},"source":["storing test data from csv file in test location  \n","\n","change test_image_location and image_path location with appropriate file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sn83vmGSTJ4Z"},"outputs":[],"source":["test_image_location=\"path...\"\n","image_path=\"..path/images/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7LZnydzzUqSj"},"outputs":[],"source":["df_t=pd.read_csv(\"/home/vikash.electron/ie643-fall-2023/test.csv\")   #modify location with appropriate location"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3wlohEJvUdEC"},"outputs":[],"source":["for im in tqdm(df_t):\n","    # get image properties\n","    image_filepath=image_path+im[0]+\".jpg\"\n","    image = Image.open(image_filepath)\n","    image.save(test_image_location+\"/images/\"+im[0]+\".jpg\")"]},{"cell_type":"markdown","metadata":{"id":"UoGcWdc3FUFq"},"source":["##Creating dataset for yolov7 training run this notebook in environment that you have created before"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"81TvvXqmKbBI"},"source":["change training dataset path to train.csv path where data is stored before and set the image location path and desired location where you want to store"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MQNeNBx0KS_b"},"outputs":[],"source":["df=pd.read_csv(\"/home/vikash.electron/ie643-fall-2023/train.csv\")   #modify with appropriate location"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IE-0sHQvK0fY"},"outputs":[],"source":["image_path=\"..path/images/\"\n","desired_location=\"..path/train/\""]},{"cell_type":"markdown","metadata":{"id":"A_9kln1dLo4j"},"source":["in desired location you have to create four folder with name \"images\", \"label\", \"Synthetic_data\" and \"train\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ThGPv8nRKQ3V"},"outputs":[],"source":["# init coco object\n","coco = Coco()\n","# append categories\n","for category_name,category_id in NAME_TO_CATEGORY_ID.items():\n","    remapped_category_id = category_id\n","    coco_category = NAME_TO_COCO_CATEGORY[category_name]\n","    coco.add_category(\n","        CocoCategory(\n","            id=int(remapped_category_id),\n","            name=coco_category[\"name\"],\n","            supercategory=coco_category[\"supercategory\"]))\n","\n","# convert visdrone annotations to coco\n","for im in tqdm(df):\n","    # get image properties\n","    image_filepath=image_path+im[0]+\".jpg\"\n","    annotation_filepath = im[1]\n","    image = Image.open(image_filepath)\n","    image.save(desired_location+\"images/\"+im[0]+\".jpg\")\n","    cocoimage_filename = im[0]+\".jpg\"\n","    coco_image = CocoImage(file_name=cocoimage_filename, height=image.size[1], width=image.size[0])\n","    lines = annotation_filepath.split(\"|\")\n","    for line in lines:\n","        # parse annotation bboxes\n","        new_line = line.split(\",\")\n","        bbox = [\n","            int(new_line[0]),\n","            int(new_line[1]),\n","            int(new_line[2]),\n","            int(new_line[3]),\n","        ]\n","        # parse category id and name\n","        category_id = NAME_TO_CATEGORY_ID[new_line[4]]\n","        category_name = new_line[4]\n","        remapped_category_id = category_id\n","        # create coco annotation and append it to coco image\n","        coco_annotation = CocoAnnotation.from_coco_bbox(\n","            bbox=bbox,\n","            category_id=int(remapped_category_id),\n","            category_name=category_name,\n","        )\n","        if coco_annotation.area > 0:\n","            coco_image.add_annotation(coco_annotation)\n","    coco.add_image(coco_image)\n","\n","save_path = desired_location+\"labels/train.json\"\n","save_json(data=coco.json, save_path=save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HFDedZFsMFn3"},"outputs":[],"source":["import fire\n","from sahi.scripts.slice_coco import slice\n","from tqdm import tqdm\n","\n","SLICE_SIZE_LIST = [480,960]\n","OVERLAP_RATIO_LIST = [0.30,0.40]\n","IGNORE_NEGATIVE_SAMPLES = False\n","\n","\n","def slice_visdrone(image_dir: str, dataset_json_path: str, output_dir: str):\n","    total_run = len(SLICE_SIZE_LIST) * len(OVERLAP_RATIO_LIST)\n","    current_run = 1\n","    for slice_size in SLICE_SIZE_LIST:\n","        for overlap_ratio in OVERLAP_RATIO_LIST:\n","            tqdm.write(\n","                f\"{current_run} of {total_run}: slicing for slice_size={slice_size}, overlap_ratio={overlap_ratio}\"\n","            )\n","            slice(\n","                image_dir=image_dir,\n","                dataset_json_path=dataset_json_path,\n","                output_dir=output_dir,\n","                slice_size=slice_size,\n","                overlap_ratio=overlap_ratio,\n","            )\n","            current_run += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YEyLT4YWMkhQ"},"outputs":[],"source":["slice_visdrone(desired_location+\"images\",desired_location+\"labels/train.json\",desired_location+\"synthetic_data\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j23AJWtIJWSk"},"outputs":[],"source":["import fire\n","from sahi.scripts.slice_coco import slice\n","from tqdm import tqdm\n","\n","SLICE_SIZE_LIST = [640,800]\n","OVERLAP_RATIO_LIST = [0, 0.25,0.35]\n","IGNORE_NEGATIVE_SAMPLES = False\n","\n","\n","def slice_visdrone(image_dir: str, dataset_json_path: str, output_dir: str):\n","    total_run = len(SLICE_SIZE_LIST) * len(OVERLAP_RATIO_LIST)\n","    current_run = 1\n","    for slice_size in SLICE_SIZE_LIST:\n","        for overlap_ratio in OVERLAP_RATIO_LIST:\n","            tqdm.write(\n","                f\"{current_run} of {total_run}: slicing for slice_size={slice_size}, overlap_ratio={overlap_ratio}\"\n","            )\n","            slice(\n","                image_dir=image_dir,\n","                dataset_json_path=dataset_json_path,\n","                output_dir=output_dir,\n","                slice_size=slice_size,\n","                overlap_ratio=overlap_ratio,\n","            )\n","            current_run += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gv2taQ-SJihw"},"outputs":[],"source":["slice_visdrone(desired_location+\"images\",desired_location+\"labels/train.json\",desired_location+\"synthetic_data/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rY-5qoEQJikp"},"outputs":[],"source":["for gen in [\"960_03\",,\"960_04\",\"640_0\",\"640_035\",\"640_025\",\"800_0\",\"800_035\",\"800_025\"]:\n","    path=desired_location+\"synthetic_data/\"+\"train_\"+gen+\".json\"\n","    with open(path, 'r') as json_file:\n","        inpu = json.load(json_file)\n","\n","    image_data = {}\n","\n","    # Iterate through the JSON data and group entries by image_id\n","    for item in inpu[\"annotations\"]:\n","        image_id = item['image_id']\n","\n","        # Check if image_id already exists in image_data dictionary\n","        if image_id in image_data:\n","            # If exists, append the entry to the existing list\n","            image_data[image_id].append(item)\n","        else:\n","            # If doesn't exist, create a new list with the current entry\n","            image_data[image_id] = [item]\n","    id_file_dict = {item['file_name']:item['id'] for item in inpu['images']}\n","    # convert visdrone annotations to coco\n","    for im in tqdm(inpu[\"images\"]):\n","        # get image properties\n","        try:\n","            image_filepath=desired_location+\"synthetic_data/\"+\"train_images_\"+gen+\"/\"+im['file_name']\n","            annotation = image_data[id_file_dict[im['file_name']]]\n","            image = Image.open(image_filepath)\n","            image.save(desired_location+\"train/images/\"+ im['file_name'].split(\".jpg\")[0]+\"_\"+gen+\".jpg\")\n","            for sample in annotation:\n","                # parse annotation bboxes\n","                new_line = sample['bbox']\n","                bbox = [\n","                    float(new_line[0]),\n","                    float(new_line[1]),\n","                    float(new_line[2]),\n","                    float(new_line[3]),\n","                ]\n","                # parse category id and name\n","                class_label = sample['category_id']\n","                yolo_bounding_box=convert_box(image.size,bbox)\n","                bounding_box_string = \" \".join([str(x) for x in yolo_bounding_box]) # Create the annotation string to be written\n","\n","                with open(desired_location+\"train/labels/\" + im['file_name'].split(\".jpg\")[0]+\"_\"+gen+\".txt\", 'a+', encoding=\"utf-8\") as output_file:\n","                    output_file.write(f\"{class_label} {bounding_box_string}\\n\")\n","        except:\n","            pass"]},{"cell_type":"markdown","metadata":{"id":"P3ajt6S8vqLI"},"source":["# YOLOv7 Training Setup\n","\n","Follow these steps to set up the YOLOv7 training environment:\n","\n","1. **Update YOLOv7 Configuration File:**\n","   - Navigate to the YOLOv7 configuration file located at `path../yolov7/data/data.yaml`.\n","   - Update the following sections:\n","\n","     ```yaml\n","     # data.yaml\n","\n","     train: desired_location + \"train/images/\"\n","     val: desired_location + \"images\"\n","     test: test_image_location + \"/images/\"\n","     ```\n","\n","     Replace `desired_location` with the desired path for training and `test_image_location` with the path for test images.\n","\n","2. **Download and Set Checkpoint for Training:**\n","   - Download the YOLOv7 checkpoint for the COCO dataset from [this link](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6e.pt).\n","   - Set the checkpoint path in your training code:\n","\n","     ```python\n","     # Your training code\n","     checkpoint_path = \"path/to/downloaded/checkpoint/yolov7-e6e.pt\"\n","     # Set the checkpoint path in your code\n","     model.load_state_dict(torch.load(checkpoint_path))\n","     ```\n","\n","     Replace `path/to/downloaded/checkpoint/` with the actual path where you downloaded the checkpoint file.\n","\n","Now, you are ready to start training your YOLOv7 model!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bUbmy674bhpD","outputId":"f880f569-c39b-408f-e5c7-2840ce3aed48"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: '/content/yolov7'\n","/content\n","--2023-11-29 17:06:25--  https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt\n","Resolving github.com (github.com)... 140.82.121.4\n","Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/511187726/13e046d1-f7f0-43ab-910b-480613181b1f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231129%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231129T170626Z&X-Amz-Expires=300&X-Amz-Signature=07a1c462c8203061c6a66239ceb7a783982aff9786c1fa0936b8303ff80deecb&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=511187726&response-content-disposition=attachment%3B%20filename%3Dyolov7_training.pt&response-content-type=application%2Foctet-stream [following]\n","--2023-11-29 17:06:26--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/511187726/13e046d1-f7f0-43ab-910b-480613181b1f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231129%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231129T170626Z&X-Amz-Expires=300&X-Amz-Signature=07a1c462c8203061c6a66239ceb7a783982aff9786c1fa0936b8303ff80deecb&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=511187726&response-content-disposition=attachment%3B%20filename%3Dyolov7_training.pt&response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 75628875 (72M) [application/octet-stream]\n","Saving to: ‘yolov7_training.pt’\n","\n","yolov7_training.pt  100%[===================>]  72.12M   220MB/s    in 0.3s    \n","\n","2023-11-29 17:06:26 (220 MB/s) - ‘yolov7_training.pt’ saved [75628875/75628875]\n","\n"]}],"source":["# download COCO starting checkpoint\n","%cd /content/yolov7\n","!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6e.pt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fONBjpSLCQZT"},"outputs":[],"source":["checkpoint_path=\"..path/yolov7-e6e.pt\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1iqOPKjr22mL"},"outputs":[],"source":["# run this cell to begin training\n","%cd /content/yolov7\n","!CUDA_VISIBLE_DEVICES=0 python train_aux.py --weights checkpoint_path --data \"data/data.yaml\" --workers 8  --batch-size 4 --img 640 640 --cfg cfg/training/yolov7-e6e.yaml --name yolov7-e6e --epochs 10 --hyp data/hyp.scratch.p6.yaml\n"]},{"cell_type":"markdown","metadata":{"id":"QOVDbKv4MB4v"},"source":["# YOLOv7 Retraining Instructions\n","\n","To further train the YOLOv7 model using the best checkpoint obtained from the previous training session, follow these steps:\n","\n","1. **Retrieve Best Checkpoint:**\n","   - The best checkpoint should be located in \"path.../yolov7/runs/train/yolov7-e6e576/weights/best.pt\".\n","\n","2. **Update Mixup Argument:**\n","   - Navigate to the configuration file at \"path..../yolov7/data/hyp.scratch.p6.yaml\".\n","   - Locate the `mixup` argument and set its value to 0.05:\n","\n","     ```yaml\n","     # hyp.scratch.p6.yaml\n","\n","     mixup: 0.05\n","     ```\n","\n","3. **Train for 10 Epochs:**\n","   - Start the training process using the updated checkpoint and mixup value.\n","   - Run the training for 10 epochs.\n","\n","Now, you are retraining the YOLOv7 model with the best checkpoint and modified mixup argument for further optimization.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZJ4FVq_QMA91"},"outputs":[],"source":["best_path=\"path/best.pt\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l7T8cXKyL9Wf"},"outputs":[],"source":["# run this cell to begin training\n","%cd /content/yolov7\n","!CUDA_VISIBLE_DEVICES=0 python train_aux.py --weights best_path --data \"data/data.yaml\" --workers 8  --batch-size 4 --img 640 640 --cfg cfg/training/yolov7-e6e.yaml --name yolov7-e6e --epochs 10 --hyp data/hyp.scratch.p6.yaml"]},{"cell_type":"markdown","metadata":{"id":"kRg1bqCjM_rB"},"source":["now by taking best checkpoint that is stored in \"path...../yolov7/runs/train/yolov7-e6e57/weights/best.pt\" train the model by below code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eIPvCzAgL9l4"},"outputs":[],"source":["best_path=\"path/best.pt\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ptdvc3uZN0ay"},"outputs":[],"source":["# run this cell to begin training\n","%cd /content/yolov7\n","!CUDA_VISIBLE_DEVICES=0 python train_aux.py --weights best_path --data \"data/data.yaml\" --workers 8  --batch-size 4 --img 960 960 --cfg cfg/training/yolov7-e6e.yaml --name yolov7-e6e --epochs 10 --hyp data/hyp.scratch.p6.yaml"]},{"cell_type":"markdown","metadata":{"id":"KJzMvRWTN_tX"},"source":["**now trainig is done we can use now best path for testing**"]},{"cell_type":"markdown","metadata":{"id":"vQ7_a4RzOYLK"},"source":["#for creating Submission file and testing\n"]},{"cell_type":"markdown","metadata":{"id":"JaN5YuUyV70-"},"source":["# Update YOLOv7 Data.yaml Configuration\n","\n","1. **Dataset Download:**\n","   - Obtain the dataset containing images and labels for training. Store the dataset in the `train` file.\n","\n","2. **Navigate to YOLOv7 Data.yaml Section:**\n","   - Go to the following path: `path../yolov7/data/data.yaml`.\n","\n","3. **Update Data.yaml Configuration:**\n","   - Find the section related to dataset configuration in the `data.yaml` file.\n","\n","4. **Set Test Image Location:**\n","   - Locate the `test` key in the configuration.\n","   - Update the `test` value to include the path to test images.\n","     ```yaml\n","     test: test_image_location+\"/images/\"\n","     ```\n","     Replace `test_image_location` with the actual path where test images are located.\n","\n","5. **Save Changes:**\n","   - Save the changes to the `data.yaml` file.\n","\n","Now, the YOLOv7 model will reference the specified path for test images during training.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F8uQNBNmOyfo"},"outputs":[],"source":["best_path=\"path.../best.pt\"  #replace with best check point"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Swcst9TXN0eR"},"outputs":[],"source":["# run this cell to begin training\n","%cd /content/yolov7\n","!CUDA_VISIBLE_DEVICES=0 python test.py --data data/data.yaml --img 1920 --batch 8 --conf 0  --iou 0.55  --device 1 --weights best_path --name yolov7_416_val --save-json --task test"]},{"cell_type":"markdown","metadata":{"id":"eghdlam5PHR0"},"source":["Above code creates a JSON file in `\"path...../yolov7/runs/test/\"` directory.\n","\n","Replace the code location below with the location of `best_predictions.json`:\n","\n","Replace this code location with the actual path of best_predictions.json\n","\n","path_to_best_predictions = \"path/to/best_predictions.json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mb_vqPiZPDuh"},"outputs":[],"source":["json_path=\"/home/vikash.electron/yolov7/runs/test/yolov7_416_val/best_predictions.json\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R4DPzOAfPDxk"},"outputs":[],"source":["import json\n","import csv\n","\n","with open(json_path, 'r') as json_file:\n","    data = json.load(json_file)\n","\n","with open('output.csv', 'w', newline='') as csv_file:\n","    csv_writer = csv.writer(csv_file)\n","    csv_writer.writerow(['image_id', 'bbox_x', 'bbox_y', 'bbox_width', 'bbox_height', 'score', 'category_id'])\n","\n","    # Write data to CSV file\n","    for item in data:\n","        image_id = item['image_id']\n","        bbox = item['bbox']\n","        score = item['score']\n","        category_id = item['category_id']\n","\n","        # Extract bbox values\n","        bbox_x, bbox_y, bbox_width, bbox_height = bbox\n","\n","        # Write the row to the CSV file\n","        csv_writer.writerow([image_id, bbox_x, bbox_y, bbox_width, bbox_height, score, category_id])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MDOxl7mpZgiS"},"outputs":[],"source":["output=pd.read_csv(\"output.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gejEhYlVXgcy"},"outputs":[],"source":["result={}\n","\n","for i in tqdm(id_file_dict.values()):\n","    df=output[output.image_id==i]\n","    result[i]=\"\"\n","    for j in np.array(df):\n","        result[i]+=(\"|\" +str(j[1])+','+str(j[2])+','+str(j[3])+','+str(j[4])+','+str(j[5])+','+CATEGORY_ID_TO_NAME[str(j[6])])\n","    result[i]=result[i][1:]\n","\n","with open(\"result.csv\", 'w', newline='') as csvfile:\n","    writer = csv.writer(csvfile)\n","    # Write the header if needed\n","    writer.writerow(['id', 'annotations'])\n","    # Write the dictionary items as rows\n","    for key, value in tqdm(result.items()):\n","        writer.writerow([key, value])"]},{"cell_type":"markdown","metadata":{"id":"ZHOunQXxXquY"},"source":["above code will create result.csv file that we can directly upload on kaggle"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
